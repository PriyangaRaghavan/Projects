{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Generating human faces.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPFWrwu9imQIkvXOQPy2kqb"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3GViq4746U5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lckUT_43zQnk"
      },
      "source": [
        "data_dir = 'gdrive/MyDrive/'\n",
        "!unzip 'gdrive/MyDrive/CelebDataset.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJ_2g-q1GwtC"
      },
      "source": [
        "import pickle as pkl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import helper\n",
        "import tests as tests"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40uw36cKzkhX"
      },
      "source": [
        "import torch\n",
        "from torchvision import datasets as dset\n",
        "from torchvision import transforms"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDpVj8vUzoNZ"
      },
      "source": [
        "def get_dataloader(batch_size, image_size, data_dir='gdrive/MyDrive/'):\n",
        "    dataset = dset.ImageFolder(root=data_dir,\n",
        "                               transform=transforms.Compose([\n",
        "                                   transforms.Resize(image_size),\n",
        "                                   transforms.ToTensor(),\n",
        "                               ]))\n",
        "    \n",
        "    data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
        "                                         shuffle=True)\n",
        "    \n",
        "    return data_loader"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lfq7Ze2IzwX_"
      },
      "source": [
        "batch_size = 16\n",
        "img_size = 32\n",
        "\n",
        "celeba_train_loader = get_dataloader(batch_size, img_size)"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThffdGbG61cp"
      },
      "source": [
        "def imshow(img):\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "\n",
        "# obtain one batch of training images\n",
        "dataiter = iter(celeba_train_loader)\n",
        "images, _ = dataiter.next() \n",
        "\n",
        "# plot the images in the batch, along with the corresponding labels\n",
        "fig = plt.figure(figsize=(20, 4))\n",
        "plot_size=20\n",
        "for idx in np.arange(plot_size):\n",
        "    ax = fig.add_subplot(2, plot_size/2, idx+1, xticks=[], yticks=[])\n",
        "    imshow(images[idx])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrwDvDdk7Jt3"
      },
      "source": [
        "def scale(x, feature_range=(-1, 1)):\n",
        "    ''' Scale takes in an image x and returns that image, scaled\n",
        "       with a feature_range of pixel values from -1 to 1. \n",
        "       This function assumes that the input x is already scaled from 0-1.'''\n",
        "    # assume x is scaled to (0, 1)\n",
        "    # scale to feature_range and return scaled x\n",
        "    \n",
        "    return x * (feature_range[1] - feature_range[0]) + feature_range[0]"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfCjF3dR7NMo"
      },
      "source": [
        "img = images[0]\n",
        "scaled_img = scale(img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icK9GH7y9BtF"
      },
      "source": [
        "Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWeXyFgB9DJ5"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPkGLLkD9GXY"
      },
      "source": [
        "def make_conv(in_channels, out_channels, kernel_size, stride=2, padding=1, batch_norm=True):\n",
        "    \n",
        "    layers=[]\n",
        "    conv_layer = nn.Conv2d(in_channels, out_channels,\n",
        "                          kernel_size, stride, padding, bias=False)\n",
        "    \n",
        "    # append conv layer\n",
        "    layers.append(conv_layer)\n",
        "    # if batch norm set to True add a batch norm layer\n",
        "    if batch_norm:\n",
        "        layers.append(nn.BatchNorm2d(out_channels))\n",
        "    \n",
        "   \n",
        "    return nn.Sequential(*layers)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGhOVD-j9RZT"
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "\n",
        "    def __init__(self, conv_dim):\n",
        "        \"\"\"\n",
        "        Initialize the Discriminator Module\n",
        "        :param conv_dim: The depth of the first convolutional layer\n",
        "        \"\"\"\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.conv_dim = conv_dim\n",
        "        # first layer : input 32 x 32 with no batch norm\n",
        "        self.conv1 = make_conv(3, conv_dim, 4, batch_norm=False)\n",
        "        # second layer : input 16 x 16 with batch norm\n",
        "        self.conv2 = make_conv(conv_dim , conv_dim*2, 4)\n",
        "        # third layer : input 8 x  8 with batch norm \n",
        "        self.conv3 = make_conv(conv_dim*2, conv_dim*4, 4)\n",
        "        # fourth layer : input 4 x 4 with batch norm \n",
        "        self.conv4 = make_conv(conv_dim*4, conv_dim*8, 4)\n",
        "\n",
        "        # fully connected layer : one output (fake/real)\n",
        "        self.fc = nn.Linear(conv_dim*8*2*2, 1)\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.leaky_relu(self.conv1(x), 0.2)\n",
        "        out = F.leaky_relu(self.conv2(out), 0.2)\n",
        "        out = F.leaky_relu(self.conv3(out), 0.2)\n",
        "        out = F.leaky_relu(self.conv4(out), 0.2)\n",
        "        \n",
        "        # flatten\n",
        "        out = out.view(-1, self.conv_dim*8*2*2)\n",
        "        \n",
        "        # final output layer\n",
        "        out = self.fc(out)        \n",
        "        return out\n",
        "\n",
        "\n",
        "tests.test_discriminator(Discriminator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWjM5nzf9ZBs"
      },
      "source": [
        "Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGVAkYYi9aZw"
      },
      "source": [
        "def make_tconv(in_channels, out_channels, kernel_size, stride=2, padding=1, batch_norm=True):\n",
        "    layers=[]\n",
        "    transpose_conv_layer = nn.ConvTranspose2d(in_channels, out_channels,\n",
        "                                             kernel_size, stride, padding, bias=False)\n",
        "    \n",
        "    # append transpose convolutional layer\n",
        "    layers.append(transpose_conv_layer)\n",
        "    \n",
        "    if batch_norm:\n",
        "        layers.append(nn.BatchNorm2d(out_channels))\n",
        "    \n",
        "    return nn.Sequential(*layers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tD7eOsOe9YkD"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    \n",
        "    def __init__(self, z_size, conv_dim):\n",
        "        super(Generator, self).__init__()\n",
        "        \n",
        "        self.conv_dim = conv_dim\n",
        "        # layers \n",
        "        # first convolutional layer : input 2 x 2  \n",
        "        self.tconv1 = make_tconv(conv_dim*8, conv_dim*4, 4)\n",
        "        #second convolutional layer : input 4 x 4  \n",
        "        self.tconv2 = make_tconv(conv_dim*4, conv_dim*2, 4)\n",
        "        # third convolutional layer : input 8 x 8 \n",
        "        self.tconv3 = make_tconv(conv_dim*2, conv_dim, 4)\n",
        "        # last convolutional layer : output 32 x 32 x 3 \n",
        "        self.tconv4 = make_tconv(conv_dim, 3, 4, batch_norm=False)\n",
        "        \n",
        "        \n",
        "        self.fc = nn.Linear(z_size, conv_dim*8*2*2)\n",
        "\n",
        "        # complete init function\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        # fully-connected + reshape \n",
        "        out = self.fc(x)\n",
        "        out = out.view(-1, self.conv_dim*8, 2, 2) # (batch_size, depth, 4, 4)\n",
        "        \n",
        "        # hidden transpose conv layers + relu\n",
        "        out = F.relu(self.tconv1(out))\n",
        "        out = F.relu(self.tconv2(out))\n",
        "        out = F.relu(self.tconv3(out))\n",
        "        \n",
        "        # last layer \n",
        "        out = self.tconv4(out)\n",
        "        # apply tanh activation\n",
        "        out = torch.tanh(out)\n",
        "        \n",
        "        return out\n",
        "\n",
        "tests.test_generator(Generator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-6ZFkFa97U9"
      },
      "source": [
        "Initialize the weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhlypTyA96yw"
      },
      "source": [
        "def weights_init_normal(m):\n",
        "    # get the class name to ensure that we initialise only for convolutional and linear layers\n",
        "    class_name = m.__class__.__name__\n",
        "    \n",
        "    if hasattr(m, 'weight') and (class_name.find('Conv') != -1 or class_name.find('Linear') != -1):\n",
        "        m.weight.data.normal_(0.0, 0.02)\n",
        "        \n",
        "        # set the bias term to 0 if it exists \n",
        "        if hasattr(m, 'bias') and m.bias is not None:\n",
        "            m.bias.data.zero_()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7U4vWpv-DZO"
      },
      "source": [
        "def build_network(d_conv_dim, g_conv_dim, z_size):\n",
        "    # define discriminator and generator\n",
        "    D = Discriminator(d_conv_dim)\n",
        "    G = Generator(z_size=z_size, conv_dim=g_conv_dim)\n",
        "\n",
        "    # initialize model weights\n",
        "    D.apply(weights_init_normal)\n",
        "    G.apply(weights_init_normal)\n",
        "\n",
        "    print(D)\n",
        "    print()\n",
        "    print(G)\n",
        "    \n",
        "    return D, G"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIEY7Caa-Ky9"
      },
      "source": [
        "# Define model hyperparams\n",
        "d_conv_dim = 128\n",
        "g_conv_dim = 128\n",
        "z_size = 100\n",
        "\n",
        "D, G = build_network(d_conv_dim, g_conv_dim, z_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hg-N1xP6-Q3_"
      },
      "source": [
        "def real_loss(D_out):\n",
        "    '''Calculates how close discriminator outputs are to being real.\n",
        "       param, D_out: discriminator logits\n",
        "       return: real loss'''\n",
        "    batch_size = D_out.size(0)\n",
        "    labels = torch.ones(batch_size)*0.9 # performed smoothing \n",
        "    \n",
        "    if train_on_gpu:\n",
        "        labels = labels.cuda()\n",
        "        \n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    loss = criterion(D_out.squeeze(),labels)\n",
        "    return loss\n",
        "\n",
        "def fake_loss(D_out):\n",
        "    '''Calculates how close discriminator outputs are to being fake.\n",
        "       param, D_out: discriminator logits\n",
        "       return: fake loss'''\n",
        "    batch_size = D_out.size(0)\n",
        "    labels = torch.zeros(batch_size) # fake images \n",
        "    \n",
        "    if train_on_gpu:\n",
        "        labels = labels.cuda()\n",
        "        \n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    loss = criterion(D_out.squeeze(),labels)\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-S4HKMjG-VXn"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Create optimizers for the discriminator D and generator G\n",
        "\n",
        "d_lr = 0.0002 \n",
        "g_lr = 0.0004 \n",
        "d_optimizer = optim.Adam(D.parameters(),d_lr, betas=(0.2, 0.999))\n",
        "g_optimizer = optim.Adam(G.parameters(),g_lr, betas=(0.2, 0.999))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrRwBOqR-bLB"
      },
      "source": [
        "def train(D, G, n_epochs, print_every=50):\n",
        "    '''Trains adversarial networks for some number of epochs\n",
        "       param, D: the discriminator network\n",
        "       param, G: the generator network\n",
        "       param, n_epochs: number of epochs to train for\n",
        "       param, print_every: when to print and record the models' losses\n",
        "       return: D and G losses'''\n",
        "    \n",
        "    # move models to GPU\n",
        "    if train_on_gpu:\n",
        "        D.cuda()\n",
        "        G.cuda()\n",
        "\n",
        "    # keep track of loss and generated, \"fake\" samples\n",
        "    samples = []\n",
        "    losses = []\n",
        "\n",
        "    # Get some fixed data for sampling. These are images that are held\n",
        "    # constant throughout training, and allow us to inspect the model's performance\n",
        "    sample_size=16\n",
        "    fixed_z = np.random.uniform(-1, 1, size=(sample_size, z_size))\n",
        "    fixed_z = torch.from_numpy(fixed_z).float()\n",
        "    # move z to GPU if available\n",
        "    if train_on_gpu:\n",
        "        fixed_z = fixed_z.cuda()\n",
        "\n",
        "    # epoch training loop\n",
        "    for epoch in range(n_epochs):\n",
        "\n",
        "        # batch training loop\n",
        "        for batch_i, (real_images, _) in enumerate(celeba_train_loader):\n",
        "\n",
        "            batch_size = real_images.size(0)\n",
        "            real_images = scale(real_images)\n",
        "\n",
        "            \n",
        "            # 1. Train the discriminator on real and fake images\n",
        "            d_optimizer.zero_grad()\n",
        "            \n",
        "            if train_on_gpu:\n",
        "                real_images = real_images.cuda()\n",
        "            \n",
        "            # loss on real images \n",
        "            d_real = D(real_images)\n",
        "            d_real_loss = real_loss(d_real)\n",
        "            \n",
        "            #train with fake images\n",
        "            z = np.random.uniform(-1, 1, size=(batch_size, z_size))\n",
        "            z = torch.from_numpy(z).float()\n",
        "            \n",
        "            if train_on_gpu:\n",
        "                z = z.cuda()\n",
        "                \n",
        "            fake_images = G(z)\n",
        "            \n",
        "            # loss on fake images\n",
        "            d_fake = D(fake_images)\n",
        "            d_fake_loss = fake_loss(d_fake)\n",
        "            \n",
        "            # backprop\n",
        "            d_loss = d_real_loss + d_fake_loss\n",
        "            d_loss.backward()\n",
        "            d_optimizer.step()\n",
        "\n",
        "            # 2. Train the generator with an adversarial loss\n",
        "            g_optimizer.zero_grad()\n",
        "            \n",
        "            # Generate fake images \n",
        "            z = np.random.uniform(-1, 1, size=(batch_size, z_size))\n",
        "            z = torch.from_numpy(z).float()\n",
        "            \n",
        "            if train_on_gpu:\n",
        "                z = z.cuda()\n",
        "                \n",
        "            fake_images = G(z)\n",
        "            d_fake = D(fake_images)\n",
        "            g_loss = real_loss(d_fake)\n",
        "            \n",
        "            # perfom backprop\n",
        "            g_loss.backward()\n",
        "            g_optimizer.step()\n",
        "         \n",
        "\n",
        "            # Print some loss stats\n",
        "            if batch_i % print_every == 0:\n",
        "                # append discriminator loss and generator loss\n",
        "                losses.append((d_loss.item(), g_loss.item()))\n",
        "                # print discriminator and generator loss\n",
        "                print('Epoch [{:5d}/{:5d}] | d_loss: {:6.4f} | g_loss: {:6.4f}'.format(\n",
        "                        epoch+1, n_epochs, d_loss.item(), g_loss.item()))\n",
        "\n",
        "\n",
        "        ## AFTER EACH EPOCH##    \n",
        "        # this code assumes your generator is named G, feel free to change the name\n",
        "        # generate and save sample, fake images\n",
        "        G.eval() # for generating samples\n",
        "        samples_z = G(fixed_z)\n",
        "        samples.append(samples_z)\n",
        "        G.train() # back to training mode\n",
        "\n",
        "    # Save training generator samples\n",
        "    with open('train_samples.pkl', 'wb') as f:\n",
        "        pkl.dump(samples, f)\n",
        "    \n",
        "    # finally return losses\n",
        "    return losses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zz3uLvZW-d_J"
      },
      "source": [
        "# set number of epochs \n",
        "n_epochs = 10\n",
        "\n",
        "# call training function\n",
        "\n",
        "losses = train(D, G, n_epochs=n_epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIPlbnJC-idq"
      },
      "source": [
        "fig, ax = plt.subplots()\n",
        "losses = np.array(losses)\n",
        "plt.plot(losses.T[0], label='Discriminator', alpha=0.5)\n",
        "plt.plot(losses.T[1], label='Generator', alpha=0.5)\n",
        "plt.title(\"Training Losses\")\n",
        "plt.legend();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqRroKqM-lfF"
      },
      "source": [
        "def view_samples(epoch, samples):\n",
        "    fig, axes = plt.subplots(figsize=(16,4), nrows=2, ncols=8, sharey=True, sharex=True)\n",
        "    for ax, img in zip(axes.flatten(), samples[epoch]):\n",
        "        img = img.detach().cpu().numpy()\n",
        "        img = np.transpose(img, (1, 2, 0))\n",
        "        img = ((img + 1)*255 / (2)).astype(np.uint8)\n",
        "        ax.xaxis.set_visible(False)\n",
        "        ax.yaxis.set_visible(False)\n",
        "        im = ax.imshow(img.reshape((32,32,3)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCjupj-t-olp"
      },
      "source": [
        "# Load samples from generator, taken while training\n",
        "with open('train_samples.pkl', 'rb') as f:\n",
        "    samples = pkl.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-sTmyWS-qY4"
      },
      "source": [
        "_ = view_samples(-1, samples)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}